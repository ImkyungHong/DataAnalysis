{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_텍스트전처리",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTD-ALNT_6bb"
      },
      "source": [
        "# 텍스트 전처리"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRhKdSrD_8s5"
      },
      "source": [
        "## 1. 토큰화(Tokenization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8i3uUG2x_wVn",
        "outputId": "e6defca5-c6fb-499f-aef0-538c5a5146b7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8aO4FcUJDXE"
      },
      "source": [
        "### 1) 표준 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebq-moqBCkYN"
      },
      "source": [
        "sample = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uY7dy6zCAz8W",
        "outputId": "db42bc7a-b090-44c0-8558-06aff222947e"
      },
      "source": [
        "from nltk.tokenize import word_tokenize  \n",
        "print(word_tokenize(sample)) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBqujfi9A4TE",
        "outputId": "d3196018-3eb2-4027-f815-ac4545172888"
      },
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "print(WordPunctTokenizer().tokenize(sample))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2L6wh8EC04I",
        "outputId": "1f266e13-fed6-428a-94cc-87889c552d89"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "print(text_to_word_sequence(sample))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymA1Cp6KIDKA"
      },
      "source": [
        "### 2) 문장 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn4PGCFGDJsC",
        "outputId": "d220de26-5872-4923-a8d3-3059e6fdb4a1"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quVqGawhIN7R",
        "outputId": "17faf60f-c2d0-4fda-fcdb-3f0e64022e0b"
      },
      "source": [
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
        "print(sent_tokenize(text))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkZlPouXIlaI"
      },
      "source": [
        "- 한글 문장 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR3DyXB-IShw",
        "outputId": "01123246-c0d4-4eab-c80e-92c0e0969189"
      },
      "source": [
        "#KSS(Korean Sentence Splitter)\n",
        "!pip install kss"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kss\n",
            "  Downloading kss-3.1.0.4.tar.gz (42.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 42.3 MB 47 kB/s \n",
            "\u001b[?25hCollecting emoji\n",
            "  Downloading emoji-1.4.2.tar.gz (184 kB)\n",
            "\u001b[K     |████████████████████████████████| 184 kB 54.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: kss, emoji\n",
            "  Building wheel for kss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kss: filename=kss-3.1.0.4-py3-none-any.whl size=42336591 sha256=ffbcecddade1f35ad6b8b7bb85918635d0e9905be44632f0e1c7da59e81e53ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/94/d8/3c/b5f02f814e08c3e2f35e32ae2ac92a34c8412ed6f92ff470ce\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186469 sha256=312d5478c523231897ece6767a9f79c91911cfc3aa5ecc04d68efcf071d472f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/61/e7/2fc1ac8f306848fc66c6c013ab511f0a39ef4b1825b11363b2\n",
            "Successfully built kss emoji\n",
            "Installing collected packages: emoji, kss\n",
            "Successfully installed emoji-1.4.2 kss-3.1.0.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zm9_yZWNIs_n",
        "outputId": "1f77e871-71c4-4697-c130-609c9403cf88"
      },
      "source": [
        "import kss\n",
        "\n",
        "text='딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요. 이제 해보면 알걸요?'\n",
        "print(kss.split_sentences(text))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Korean Sentence Splitter]: Initializing Kss...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담아니에요.', '이제 해보면 알걸요?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK8LksTbKOCB"
      },
      "source": [
        "### 3) 품사(POS) 태깅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EllcgmXtI0eK",
        "outputId": "8fd0cbef-f9f5-4e1a-ed70-aadbc5bdef22"
      },
      "source": [
        "text=\"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiD1o-bKKmoz",
        "outputId": "c27c4116-ca5b-4888-e89c-6f43ce433d6b"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qkYsUxYKcVY",
        "outputId": "6a221ede-bbda-45cc-e790-f4284beaede3"
      },
      "source": [
        "from nltk.tag import pos_tag\n",
        "x=word_tokenize(text)\n",
        "pos_tag(x)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('I', 'PRP'),\n",
              " ('am', 'VBP'),\n",
              " ('actively', 'RB'),\n",
              " ('looking', 'VBG'),\n",
              " ('for', 'IN'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('students', 'NNS'),\n",
              " ('.', '.'),\n",
              " ('and', 'CC'),\n",
              " ('you', 'PRP'),\n",
              " ('are', 'VBP'),\n",
              " ('a', 'DT'),\n",
              " ('Ph.D.', 'NNP'),\n",
              " ('student', 'NN'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJjF38lQK502"
      },
      "source": [
        "- 한글 (KoNLPy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IJj8xGCKfCB",
        "outputId": "883a5bdc-e017-4f9b-c031-79451de0abc8"
      },
      "source": [
        "!pip install KoNlPY "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting KoNlPY\n",
            "  Downloading konlpy-0.5.2-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from KoNlPY) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 52.3 MB/s \n",
            "\u001b[?25hCollecting beautifulsoup4==4.6.0\n",
            "  Downloading beautifulsoup4-4.6.0-py3-none-any.whl (86 kB)\n",
            "\u001b[K     |████████████████████████████████| 86 kB 5.9 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from KoNlPY) (1.19.5)\n",
            "Requirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from KoNlPY) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->KoNlPY) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->KoNlPY) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->KoNlPY) (1.3.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->KoNlPY) (2.23.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->KoNlPY) (3.1.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNlPY) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNlPY) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNlPY) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNlPY) (2021.5.30)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNlPY) (1.7.1)\n",
            "Installing collected packages: JPype1, colorama, beautifulsoup4, KoNlPY\n",
            "  Attempting uninstall: beautifulsoup4\n",
            "    Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.3.0 KoNlPY-0.5.2 beautifulsoup4-4.6.0 colorama-0.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaCPWYfpM11Q"
      },
      "source": [
        "- Okt "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1zj47riLHlF",
        "outputId": "26f740ac-dcb4-4127-bede-ee3a55c25574"
      },
      "source": [
        "# 형태소 분석\n",
        "from konlpy.tag import Okt  \n",
        "okt=Okt()   #클래스니까 객체 생성 필요\n",
        "okt.morphs('열심히 코딩한 당신, 연휴에는 여행을 가봐요')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5wQ2QbwL1kw",
        "outputId": "dae758a3-1d3e-4f98-d349-d791fa3907e3"
      },
      "source": [
        "# 품사 부착\n",
        "text = '열심히 코딩한 당신, 연휴에는 여행을 가봐요'\n",
        "okt.pos(text)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('열심히', 'Adverb'),\n",
              " ('코딩', 'Noun'),\n",
              " ('한', 'Josa'),\n",
              " ('당신', 'Noun'),\n",
              " (',', 'Punctuation'),\n",
              " ('연휴', 'Noun'),\n",
              " ('에는', 'Josa'),\n",
              " ('여행', 'Noun'),\n",
              " ('을', 'Josa'),\n",
              " ('가봐요', 'Verb')]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-spFvCFbMOXv",
        "outputId": "8499ff22-75b6-4423-b9f9-371988d0468c"
      },
      "source": [
        "# 명사 추출\n",
        "okt.nouns(text)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BF8fdub2M8Ym"
      },
      "source": [
        "- 꼬꼬마 형태소 분석기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-xDPdSHMZ3f",
        "outputId": "9dd73040-d59a-449c-c613-4f277d905c16"
      },
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "kkma.morphs(text)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbHy7c8DNIRK",
        "outputId": "3a8d486a-1ff2-4573-c086-11064ee55255"
      },
      "source": [
        "kkma.pos(text)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('열심히', 'MAG'),\n",
              " ('코딩', 'NNG'),\n",
              " ('하', 'XSV'),\n",
              " ('ㄴ', 'ETD'),\n",
              " ('당신', 'NP'),\n",
              " (',', 'SP'),\n",
              " ('연휴', 'NNG'),\n",
              " ('에', 'JKM'),\n",
              " ('는', 'JX'),\n",
              " ('여행', 'NNG'),\n",
              " ('을', 'JKO'),\n",
              " ('가보', 'VV'),\n",
              " ('아요', 'EFN')]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOhOa8RLN9B5",
        "outputId": "8a52258c-509a-40cf-e2c1-dab3dd73668a"
      },
      "source": [
        "kkma.nouns(text)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['코딩', '당신', '연휴', '여행']"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-_PA-ATOQKA"
      },
      "source": [
        "## 2. 정제 및 정규화 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwpee1asOBjJ",
        "outputId": "3f7e76b3-b6f2-49b4-e2c0-0a4007968101"
      },
      "source": [
        "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
        "import re\n",
        "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
        "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
        "print(shortword.sub('', text))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " was wondering anyone out there could enlighten this car.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMJEVZ1YQzJ2"
      },
      "source": [
        "## 3. 어간 추출(Stemming) and 표제어 추출(Lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EI1rTVnQ8F6"
      },
      "source": [
        "### 1) 표제어 추출(Lemmatization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uGw1N_H9RYiU",
        "outputId": "e2e77516-de97-4eb0-f8f6-9abb764a5793"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yMiNajjNQhQj",
        "outputId": "d4fc9fd7-4f83-4576-f1e0-bf1f86601928"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "n=WordNetLemmatizer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([n.lemmatize(w) for w in words])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'starting']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "173yHQisRVF5",
        "outputId": "4f5854ae-1882-41d7-de33-ca95de7add24"
      },
      "source": [
        "n.lemmatize('dies','v'), n.lemmatize('has','v'), n.lemmatize('watched','v')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('die', 'have', 'watch')"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-wzHEolSRYH"
      },
      "source": [
        "### 2) 어간 추출(Stemming)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epzes7R5SFxE",
        "outputId": "2d4dee8f-fb35-4294-b4bf-ed75d1d2da97"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "s = PorterStemmer()\n",
        "text=\"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\"\n",
        "words=word_tokenize(text)\n",
        "print(words)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYaUnh4kSed1",
        "outputId": "e1984e55-6691-40d9-83df-e5e6ef001119"
      },
      "source": [
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWxP8YIJTYtx",
        "outputId": "0f983cc2-d09c-41b8-d7da-9aa759711dd0"
      },
      "source": [
        "words=['formalize', 'allowance', 'electricical']\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['formal', 'allow', 'electric']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUyR81vpTM_P",
        "outputId": "0f543ba6-af29-4b55-bb8b-d6dc5a3fc612"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "s=PorterStemmer()\n",
        "words=['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'starting']\n",
        "print([s.stem(w) for w in words])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['polici', 'do', 'organ', 'have', 'go', 'love', 'live', 'fli', 'die', 'watch', 'ha', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OF_beNMQTEyS",
        "outputId": "6cc7ff5b-7d3c-450e-f1fe-ca2784623edb"
      },
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "l=LancasterStemmer()\n",
        "print([l.stem(w) for w in words])"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['policy', 'doing', 'org', 'hav', 'going', 'lov', 'liv', 'fly', 'die', 'watch', 'has', 'start']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lsv-2kSKVz_M"
      },
      "source": [
        "## 4. 불용어(Stopword)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wv-zRxgTTOSU",
        "outputId": "44e20775-f71a-4a1c-e03c-f73fe042674d"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "len(stopwords.words('english'))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lANFlR2RWUWV",
        "outputId": "27429094-7fda-4635-b6f0-2f9206e3c359"
      },
      "source": [
        "stopwords.words('english')[:10]"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soFfpSZTWdkk",
        "outputId": "42aab8d4-7f28-4a1e-ed23-e683a2292b88"
      },
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = []\n",
        "for w in word_tokens: \n",
        "    if w not in stop_words: \n",
        "        result.append(w) \n",
        "\n",
        "print(word_tokens) \n",
        "print(result) "
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Family', 'is', 'not', 'an', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n",
            "['Family', 'important', 'thing', '.', 'It', \"'s\", 'everything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0wmV58EXJku",
        "outputId": "6219de4d-2132-49a5-ea7e-0967627c1ebb"
      },
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "\n",
        "word_tokens = word_tokenize(example.lower())    #소문자로\n",
        "\n",
        "result = [w for w in word_tokens if w not in stop_words]    #list comprehension\n",
        "\n",
        "print(word_tokens) \n",
        "print(result) "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['family', 'is', 'not', 'an', 'important', 'thing', '.', 'it', \"'s\", 'everything', '.']\n",
            "['family', 'important', 'thing', '.', \"'s\", 'everything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EknJEPvFXPeF",
        "outputId": "2759adbb-0815-477e-aa49-a5b8c12b9873"
      },
      "source": [
        "example = \"Family is not an important thing. It's everything.\"\n",
        "stop_words = set(stopwords.words('english')) \n",
        "stop_words.add(\"'s\")    #추가하고 싶으면 추가하면 됨\n",
        "\n",
        "word_tokens = word_tokenize(example.lower())   \n",
        "\n",
        "result = [w for w in word_tokens if w not in stop_words]\n",
        "\n",
        "print(word_tokens) \n",
        "print(result) "
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['family', 'is', 'not', 'an', 'important', 'thing', '.', 'it', \"'s\", 'everything', '.']\n",
            "['family', 'important', 'thing', '.', 'everything', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmyJEH5EXvkq",
        "outputId": "957f1e37-da73-420e-b3ab-b7c3ea52a10e"
      },
      "source": [
        "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
        "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든\"\n",
        "# 위의 불용어는 명사가 아닌 단어 중에서 저자가 임의로 선정한 것으로 실제 의미있는 선정 기준이 아님\n",
        "stop_words=stop_words.split(' ')\n",
        "word_tokens = word_tokenize(example)\n",
        "\n",
        "result = [w for w in word_tokens if w not in stop_words]\n",
        "\n",
        "print(word_tokens) \n",
        "print(result)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['고기를', '아무렇게나', '구우려고', '하면', '안', '돼', '.', '고기라고', '다', '같은', '게', '아니거든', '.', '예컨대', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n",
            "['고기를', '구우려고', '안', '돼', '.', '고기라고', '다', '같은', '게', '.', '삼겹살을', '구울', '때는', '중요한', '게', '있지', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JduZLyK7X_8X"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}